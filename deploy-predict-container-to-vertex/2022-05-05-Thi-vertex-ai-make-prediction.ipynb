{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8631184-8ef3-455f-84cf-f1c758962c1e",
   "metadata": {},
   "source": [
    "Use the idea in [this notebook](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb) => we will create a custom container for the prediction step. \n",
    "\n",
    "- That idea uses another model which doesn't need `pipeline()` to give the prediction.\n",
    "- In our customization, we will use `pipeline()` method in the prediction step of the container.\n",
    "\n",
    "__Warning__: In this notebook, I use the same variable names in different sections with or without the same values. Make sure you have the right things before running the codes.\n",
    "\n",
    "__TL;DR__:\n",
    "\n",
    "The final model is the one created at step \"Option 2: Using `tokenizer` to encode and then decode to text to be used in `inference` with `pipeline()`\". They uses:\n",
    "\n",
    "- `predictor/custom_handler_3.py` \n",
    "- `Dockerfile.3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240aa2e-d575-45d8-b32b-b0c2d0ac92e6",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27a681c-f236-4cda-8c21-e5874c6e6659",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62932f6c-43cb-4a58-b802-b268788d25ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a075764-1f66-4f7a-9f75-18103a3fcd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The Google Cloud Notebook product has specific requirements\n",
    "# IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# # Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "# USER_FLAG = \"\"\n",
    "# if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "#     USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f002e4a9-4ec5-4de9-ad77-f44c97e5e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Vertex AI SDK\n",
    "# !pip -q install {USER_FLAG} --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f672cc52-3214-43db-b8de-d319e5df7b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To disable the warning:\n",
    "# huggingface/tokenizers: The current process just got forked, after parallelism has already been used.\n",
    "# Disabling parallelism to avoid deadlocks...\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c68c669-f671-4f92-b317-58cada9bd53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from google.cloud import aiplatform\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a18b322-7a4d-4028-a9ac-42669844406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e5b1543-293f-4803-a5b2-3cea5843cf5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook runtime: CPU\n",
      "PyTorch version : 1.11.0+cu102\n",
      "Transformers version : 4.18.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Notebook runtime: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"PyTorch version : {torch.__version__}\")\n",
    "print(f\"Transformers version : {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af63da54-a122-4b01-ae7d-e47be9dc6e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.ideta-ml-thi.appspot.com/\n",
      "gs://ideta-sentiment-analysis/\n",
      "gs://torch-text-class-testing/\n"
     ]
    }
   ],
   "source": [
    "# Link with google storage\n",
    "! gsutil ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338ba791-b619-4866-82bc-d4d8bc8f0a2d",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b88fb8-e8a0-4f15-bbcd-8c62cee88e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_NAME = 'gs://ideta-sentiment-analysis/'\n",
    "PROJECT_ID = 'ideta-ml-thi'\n",
    "REGION = 'europe-west1'\n",
    "src_dir = \"../../..\" # change this if your notebook is in another place\n",
    "data_loc = src_dir + \"/model\"\n",
    "APP_NAME = 'pt-xlm-roberta-large-xnli'\n",
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "data_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f88b293-f946-4ac6-b30c-47f5c0ccd78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../model/pt-xlm-roberta-large-xnli'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model_dir = data_loc + '/' + APP_NAME\n",
    "pt_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d9927c-5ffe-4ff9-8114-d191bac97e47",
   "metadata": {},
   "source": [
    "# From and to Torch format\n",
    "\n",
    "Runce once, if you've already saved the model, skip to next section to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21c888fe-28a7-429c-b7b2-be40914a38ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../model/pt-xlm-roberta-large-xnli'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c816b-a392-41d1-9f24-21bd6d1347d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "8da7a798f8f445748d80faa6b9aa7518",
      "bc4596805467485eb7ac2df64247b11b",
      "080e0f57e5854306bf11cd3f70a4645c",
      "3f6f9328b4cf4e7f91032ffb1a969ec8",
      "9e4e4f561892416f860a342d8e5c6e6d",
      "54ab6731183e43aabf5f3d5d932264d6",
      "d8d5de5ff0f04f009cfe6f26e8fbea62",
      "ef3833ca0309445c9a971c4790ea3403",
      "47863a3797ca4bb1b9ab709e530204c2",
      "3ad132cf4af345c38fdd6fe95e5f0b8a",
      "d0b9ebc5b3ac4fcc84782f76a3e6585c",
      "ea566c70c2b349d3a51fdf48658cf7e8",
      "5f0606f6396749b5bdea813c6b22320c",
      "25c66211316a4f11a6c8bd1079320de9",
      "5c4aac8ce54c4070bcf9b187f2b839a0",
      "b232e608bdf74d99bae0f84a2d6ad037",
      "bc9fef495c5d4003831553b3e0f42ef2",
      "1570e15dc90d4ce2978cb0647a1b8698",
      "d50d84aa3de7458d9547dd27c9511ff8",
      "4bb40eb3cc5c4e8cb7a8a315943fe111",
      "e18333d1aa2c4fc1a6abe7f3e80b94e7",
      "af56de88f4864c55a72feccfb96c8e67",
      "907dfe39b83d48eb8a147118225b49db",
      "10c270fb9003488eac550a8acc66161b",
      "81d23b5eea2c41129e108019e11cc3bc",
      "7ac7c4e93c0141d89ef26ca957477c61",
      "9397c4fd25c84e018e0e120d75256003",
      "deb4914e78b14f7aa4352327da3b25b9",
      "42530c5ba1994daeb7fd6ec3580b6222",
      "104a7ca87c0c4997840f82f8ea139cd9",
      "ba8be4d0267a4a1599babb0158bc72d4",
      "7dac7647a92f4aa9a5b6010759d4ef58",
      "46319b15fca448baa49b3c8ce92c7cb1",
      "0735490b9a624104a176074915272801",
      "c69a17861c444addb6f9f89ae98a61f9",
      "f96565b6a62a4a40a35c2cdce1452fc3",
      "98cffb876bbd415c95e16550fba6933f",
      "dd474a1914b643ddb00b6ae254e11bc3",
      "6c852c946b0a48eea49da4a052e781b6",
      "d87b762ea6c04852a1ef2ea0d4048daf",
      "06e883b4476c47cc8d874213dd3102cb",
      "3b4b644c420641fb9cef11d7781e3f44",
      "fba16faf10f74cd5acbca56f1f1ad789",
      "cb870e7fd1f04830a60db4689af3d4c7"
     ]
    },
    "id": "WABGMuLk4gBI",
    "outputId": "c4eb63d8-f04f-42c3-e999-03e04bec2a44"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "470a6358-62f7-44a5-9430-459c33bc7df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5a763ea291a4d03aa3e7e663057581a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1412f-7d61-45d1-9d0c-285048e25c39",
   "metadata": {},
   "source": [
    "Save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d2aa339-5fb1-47f7-9619-97b0b4560a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../../model/pt-xlm-roberta-large-xnli/tokenizer_config.json',\n",
       " '../../../model/pt-xlm-roberta-large-xnli/special_tokens_map.json',\n",
       " '../../../model/pt-xlm-roberta-large-xnli/sentencepiece.bpe.model',\n",
       " '../../../model/pt-xlm-roberta-large-xnli/added_tokens.json',\n",
       " '../../../model/pt-xlm-roberta-large-xnli/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(pt_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f6471b2-e4a3-4d75-acfa-92fdadbbd93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model.save_pretrained(pt_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16874d57-5ed0-42f4-a214-ff87c3087840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   sentencepiece.bpe.model  tokenizer.json\n",
      "pytorch_model.bin  special_tokens_map.json  tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "! ls {pt_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81eb3b4-55fc-4a68-9726-b6bf5e40cb02",
   "metadata": {},
   "source": [
    "# Load from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c65452f-3eae-44c3-9552-c5fdb4cadae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../model/pt-xlm-roberta-large-xnli'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a67732-a7a4-4ed2-82e8-d8b8d571c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_tokenizer = AutoTokenizer.from_pretrained(pt_model_dir)\n",
    "saved_model = AutoModelForSequenceClassification.from_pretrained(pt_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea5650-13f0-453c-a553-fb241aa80f4b",
   "metadata": {},
   "source": [
    "## Use `pipeline` to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f358e33-4b01-47c4-871c-610fda88f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='zero-shot-classification', model=saved_model, tokenizer=saved_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f443e26b-5ee1-4e62-b5ce-88b368ca0625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"I'm really disapointed with what are you saying and your service, give my money back!!!!\",\n",
       " 'labels': ['negative', 'neutral', 'positive'],\n",
       " 'scores': [0.9862303137779236, 0.00949057936668396, 0.004279125016182661]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    'I\\'m really disapointed with what are you saying and your service, give my money back!!!!',\n",
    "    candidate_labels=[\"positive\", \"negative\", \"neutral\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d903dff-902c-4166-ba59-8d8359474f46",
   "metadata": {},
   "source": [
    "# Create a custom image for prediction\n",
    "\n",
    "https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements\n",
    "\n",
    "We use `TorchServe`! => https://pytorch.org/serve/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b08888-ca2a-4083-b7bd-485691c1f9ce",
   "metadata": {},
   "source": [
    "## Play with functions in TorchServe before making the `custom_handler.py` file\n",
    "\n",
    "Source code without using `pipeline()`: https://github.com/dinhanhthi/google-vertex-ai/tree/main/VIEW_ONLY_pytorch_text_classification_using_vertex_sdk_and_gcloud/predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14fc1d-7b5a-4fbd-90b7-97f889c6e093",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/serve/api/ts.torch_handler.html#ts.torch_handler.base_handler.BaseHandler.preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "118d6448-1e59-4bc5-b05e-ce9845f900f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, data):\n",
    "    \"\"\" Preprocessing input request by tokenizing\n",
    "        Extend with your own preprocessing steps as needed\n",
    "    \"\"\"\n",
    "    text = data[0].get(\"data\")\n",
    "    if text is None:\n",
    "        text = data[0].get(\"body\")\n",
    "    sentences = text.decode('utf-8')\n",
    "    print(\"Received text: '%s'\", sentences)\n",
    "\n",
    "    # Tokenize the texts\n",
    "    tokenizer_args = ((sentences,))\n",
    "    inputs = tokenizer(*tokenizer_args,\n",
    "                        padding='max_length',\n",
    "                        max_length=128,\n",
    "                        truncation=True,\n",
    "                        return_tensors = \"pt\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b48ce0c-83ba-4894-880e-ff259c42765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1991a126-a8cb-44d7-9e15-7edacff99f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    {\n",
    "        \"data\": b\"Jaw dropping visual affects and action! One of the best I have seen to date.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664aff1b-d8d7-4557-a2a6-cd5c2aab7e31",
   "metadata": {},
   "source": [
    "What is `b`? => https://stackoverflow.com/questions/6269765/what-does-the-b-character-do-in-front-of-a-string-literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "194f51d0-8dee-47c9-b620-3f6fa675b6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received text: '%s' Jaw dropping visual affects and action! One of the best I have seen to date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   823,   434, 36069, 10366, 21176, 52490,     7,   136, 22631,\n",
       "            38,  6561,   111,    70,  2965,    87,   765, 51592,    47,  5622,\n",
       "             5,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = preprocess(saved_tokenizer, test_data)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b2389c53-a189-4933-8139-9da5c4a7216f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,   823,   434, 36069, 10366, 21176, 52490,     7,   136, 22631,\n",
       "           38,  6561,   111,    70,  2965,    87,   765, 51592,    47,  5622,\n",
       "            5,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "            1,     1,     1,     1,     1,     1,     1,     1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1abace12-f77f-455e-a622-84f598eb4858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jaw dropping visual affects and action! One of the best I have seen to date.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e9163-adbe-439d-bfd5-67e551658dce",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/serve/api/ts.torch_handler.html#ts.torch_handler.base_handler.BaseHandler.inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9ca56191-337d-45b8-baaf-e75ed829580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "def inference(model, inputs):\n",
    "    \"\"\" Predict the class of a text using a trained transformer model.\n",
    "    \"\"\"\n",
    "    prediction = model(inputs['input_ids'].to(device))[0].argmax().item()\n",
    "        \n",
    "    print(\"Model predicted: '%s'\", prediction)\n",
    "    return [prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0cefcf2-0f03-4673-9c22-beb3aa720f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predicted: '%s' 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(saved_model, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10b6bc9e-2d3f-41c4-b415-f0aa55a1e691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-2.0831,  2.5675, -0.9757]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model(inputs['input_ids'].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47272f3c-873a-4c41-acc9-e0697a977c61",
   "metadata": {},
   "source": [
    "❗ Because the `reference()` requires input as `Torch Tensor` type but the `pipeline()` requires the sentence to be `str`. We have to find a way to convert from Torch Tensor to str.\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline\n",
    "\n",
    "We use this pipeline: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.ZeroShotClassificationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0effe45d-6fb9-4866-87dd-b6abe5bdec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task='zero-shot-classification', model=saved_model, tokenizer=saved_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "633691b0-e57e-46b5-a498-23ec6ca1d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jaw dropping visual affects and action! One of the best I have seen to date.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff4808f4-3339-47ce-b833-374828aaef69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'Jaw dropping visual affects and action! One of the best I have seen to date.',\n",
       " 'labels': ['positive', 'negative', 'neutral'],\n",
       " 'scores': [0.9401667714118958, 0.034984465688467026, 0.024848783388733864]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\n",
    "    inputs[0],\n",
    "    candidate_labels=[\"positive\", \"negative\", \"neutral\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb0807-fcba-414c-9bab-b4682f19f90f",
   "metadata": {},
   "source": [
    "👉 Try this idea: https://github.com/cceyda/lit-NER/blob/master/lit_ner/serve.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9104af2-de1c-4c55-8a5b-b3a77b5b4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "saved_model = AutoModelForSequenceClassification.from_pretrained(pt_model_dir)\n",
    "saved_model.to(device)\n",
    "saved_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ce0cb72-b24c-4438-99f9-2002f468dafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received text: '%s' Jaw dropping visual affects and action! One of the best I have seen to date.\n"
     ]
    }
   ],
   "source": [
    "def preprocess(tokenizer, data):\n",
    "    \"\"\" Preprocessing input request by tokenizing\n",
    "        Extend with your own preprocessing steps as needed\n",
    "    \"\"\"\n",
    "    text = data[0].get(\"data\")\n",
    "    if text is None:\n",
    "        text = data[0].get(\"body\")\n",
    "    sentences = text.decode('utf-8')\n",
    "    print(\"Received text: '%s'\", sentences)\n",
    "    \n",
    "    # Below: https://github.com/cceyda/lit-NER/blob/master/lit_ner/serve.py\n",
    "    processed_sentences = []\n",
    "    num_separated = [s.strip() for s in re.split(\"(\\d+)\", sentences)]\n",
    "    digit_processed = \" \".join(num_separated)\n",
    "    processed_sentences.append(digit_processed)\n",
    "    \n",
    "    return processed_sentences\n",
    "\n",
    "inputs = preprocess(saved_tokenizer, test_data)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "def inference(model, tokenizer, inputs):\n",
    "    \"\"\" Predict the class of a text using a trained transformer model.\n",
    "    \"\"\"\n",
    "    pipe = pipeline(task='zero-shot-classification', model=model, tokenizer=tokenizer)\n",
    "    prediction = pipe(inputs[0], candidate_labels=[\"positive\", \"negative\", \"neutral\"])\n",
    "        \n",
    "    return [prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb31e9e0-ad4e-49a9-af49-549963cd6f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'Jaw dropping visual affects and action! One of the best I have seen to date.',\n",
       "  'labels': ['positive', 'negative', 'neutral'],\n",
       "  'scores': [0.9401667714118958, 0.034984465688467026, 0.024848783388733864]}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(saved_model, saved_tokenizer, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43976ae9-edcf-46aa-a867-54e0ac74796e",
   "metadata": {},
   "source": [
    "## Create `custom_hander.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b465de-9a42-48a6-b2b9-59447ec368e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3b3f92ef-acb0-4c50-aae0-55faa5433434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing predictor/custom_handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictor/custom_handler.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformersClassifierHandler(BaseHandler):\n",
    "    \"\"\"\n",
    "    The handler takes an input string and returns the classification text \n",
    "    based on the serialized transformers checkpoint.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TransformersClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        \"\"\" Loads the model.pt file and initialized the model object.\n",
    "        Instantiates Tokenizer for preprocessor to use\n",
    "        Loads labels to name mapping file for post-processing inference response\n",
    "        \"\"\"\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest[\"model\"][\"serializedFile\"]\n",
    "        model_pt_path = os.path.join(model_dir, serialized_file)\n",
    "        if not os.path.isfile(model_pt_path):\n",
    "            raise RuntimeError(\"Missing the model.pt or pytorch_model.bin file\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        logger.debug('Transformer model from path {0} loaded successfully'.format(model_dir))\n",
    "        \n",
    "        # Ensure to use the same tokenizer used during training\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "        # pipeline()\n",
    "        self.pipe = pipeline(task='zero-shot-classification', model=self.model, tokenizer=self.tokenizer)\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\" Preprocessing input request by tokenizing\n",
    "            Extend with your own preprocessing steps as needed\n",
    "        \"\"\"\n",
    "        text = data[0].get(\"data\")\n",
    "        if text is None:\n",
    "            text = data[0].get(\"body\")\n",
    "        sentences = text.decode('utf-8')\n",
    "        logger.info(\"Received text: '%s'\", sentences)\n",
    "\n",
    "        # Below: https://github.com/cceyda/lit-NER/blob/master/lit_ner/serve.py\n",
    "        processed_sentences = []\n",
    "        num_separated = [s.strip() for s in re.split(\"(\\d+)\", sentences)]\n",
    "        digit_processed = \" \".join(num_separated)\n",
    "        processed_sentences.append(digit_processed)\n",
    "\n",
    "        return processed_sentences\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\" Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        prediction = self.pipe(inputs[0], candidate_labels=[\"negative\", \"neutral\", \"positive\"])\n",
    "        if len(inputs) == 1:\n",
    "            prediction = [prediction]\n",
    "        return prediction\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        return inference_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7137e85-7b01-41c8-b124-6401e257cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model to /predictor/\n",
    "! cp {src_dir}/model/pt-xlm-roberta-large-xnli {src_dir}/ideta-logos/playground/vertex-ai/predictor/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6c0bf-6793-4a5f-8ae8-867d00de22bc",
   "metadata": {},
   "source": [
    "### Make some tests to understanding the last steps of `preprocess()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bb27a203-ab49-4f32-985e-7b5acddf7389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am not ', '1', ' happy.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instance = b\"I am really disapointed with your service!\"\n",
    "# text = base64.b64encode(instance)\n",
    "# sentences = text.decode('utf-8')\n",
    "\n",
    "# print(\"Received text: '%s'\", sentences)\n",
    "\n",
    "sentences = \"I am not 1 happy.\"\n",
    "re.split(\"(\\d+)\", sentences)\n",
    "\n",
    "\n",
    "# # Below: https://github.com/cceyda/lit-NER/blob/master/lit_ner/serve.py\n",
    "# processed_sentences = []\n",
    "# num_separated = [s.strip() for s in re.split(\"(\\d+)\", sentences)]\n",
    "# digit_processed = \" \".join(num_separated)\n",
    "# processed_sentences.append(digit_processed)\n",
    "# print(f\"processed_sentences: {processed_sentences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4bbba4-8388-43d2-a83d-777b88e95859",
   "metadata": {},
   "source": [
    "## Create `Dockerfile` file\n",
    "\n",
    "This is done manually but you can use below shell to perform this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79eff1-ebee-482a-b657-3427bd1a8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat << EOF > ./predictor/Dockerfile\n",
    "\n",
    "FROM pytorch/torchserve:latest-cpu\n",
    "\n",
    "# install dependencies\n",
    "RUN python3 -m pip install --upgrade pip\n",
    "RUN pip3 install transformers\n",
    "\n",
    "USER model-server\n",
    "\n",
    "# copy model artifacts, custom handler and other dependencies\n",
    "COPY ./custom_handler.py /home/model-server/\n",
    "COPY ./model/$APP_NAME/ /home/model-server/\n",
    "\n",
    "# create torchserve configuration file\n",
    "USER root\n",
    "RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
    "RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
    "USER model-server\n",
    "\n",
    "# expose health and prediction listener ports from the image\n",
    "EXPOSE 7080\n",
    "EXPOSE 7081\n",
    "\n",
    "# create model archive file packaging model artifacts and dependencies\n",
    "RUN torch-model-archiver -f \\\n",
    "  --model-name=$APP_NAME \\\n",
    "  --version=1.0 \\\n",
    "  --serialized-file=/home/model-server/pytorch_model.bin \\\n",
    "  --handler=/home/model-server/custom_handler.py \\\n",
    "  --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json\" \\\n",
    "  --export-path=/home/model-server/model-store\n",
    "\n",
    "# run Torchserve HTTP serve to respond to prediction requests\n",
    "CMD [\"torchserve\", \\\n",
    "     \"--start\", \\\n",
    "     \"--ts-config=/home/model-server/config.properties\", \\\n",
    "     \"--models\", \\\n",
    "     \"$APP_NAME=$APP_NAME.mar\", \\\n",
    "     \"--model-store\", \\\n",
    "     \"/home/model-server/model-store\"]\n",
    "EOF\n",
    "\n",
    "echo \"Writing ./predictor/Dockerfile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393a8e8-1707-4d44-bebd-08a4991d5d37",
   "metadata": {},
   "source": [
    "## Create custom prediction image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b98aaa68-3d3c-4993-8110-c72045d3f970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3ed2a99f-8642-429f-9814-1c03c750fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.262GB\n",
      "Step 1/15 : FROM pytorch/torchserve:latest-cpu\n",
      " ---> a1d88b873573\n",
      "Step 2/15 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 9cd7983a39ee\n",
      "Step 3/15 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> 05d7d7d7e863\n",
      "Step 4/15 : USER model-server\n",
      " ---> Using cache\n",
      " ---> c989f33b4974\n",
      "Step 5/15 : COPY ./custom_handler.py /home/model-server/\n",
      " ---> adcbbbd31662\n",
      "Step 6/15 : COPY ./model/pt-xlm-roberta-large-xnli/ /home/model-server/\n",
      " ---> 193b21a0082a\n",
      "Step 7/15 : USER root\n",
      " ---> Running in 6135f43a34d3\n",
      "Removing intermediate container 6135f43a34d3\n",
      " ---> 00b417261432\n",
      "Step 8/15 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in ea3b76e25975\n",
      "Removing intermediate container ea3b76e25975\n",
      " ---> f0323076c9ea\n",
      "Step 9/15 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in df8de4368c21\n",
      "Removing intermediate container df8de4368c21\n",
      " ---> 8254f6c036e3\n",
      "Step 10/15 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 10892af3645f\n",
      "Removing intermediate container 10892af3645f\n",
      " ---> 5eab00f51882\n",
      "Step 11/15 : USER model-server\n",
      " ---> Running in 475a07dc0223\n",
      "Removing intermediate container 475a07dc0223\n",
      " ---> dbe9c3acef50\n",
      "Step 12/15 : EXPOSE 7080\n",
      " ---> Running in 875602593d1a\n",
      "Removing intermediate container 875602593d1a\n",
      " ---> 38ea6d71c7d5\n",
      "Step 13/15 : EXPOSE 7081\n",
      " ---> Running in 9165aa2ccbdc\n",
      "Removing intermediate container 9165aa2ccbdc\n",
      " ---> 58721bb32e7a\n",
      "Step 14/15 : RUN torch-model-archiver -f   --model-name=pt-xlm-roberta-large-xnli   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in c99dd1b51ae0\n",
      "Removing intermediate container c99dd1b51ae0\n",
      " ---> d260c227ec91\n",
      "Step 15/15 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"pt-xlm-roberta-large-xnli=pt-xlm-roberta-large-xnli.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in 3634a70611c7\n",
      "Removing intermediate container 3634a70611c7\n",
      " ---> b8554804544d\n",
      "Successfully built b8554804544d\n",
      "Successfully tagged gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli:latest\n"
     ]
    }
   ],
   "source": [
    "# Build based on predictor/custom_handler.py and predictor/Dockerfile\n",
    "!docker build \\\n",
    "  --tag=$CUSTOM_PREDICTOR_IMAGE_URI \\\n",
    "  ./predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae6af9-a76b-4ceb-829b-22710a298b78",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the predict container LOCALLY before pushing to vertex registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8027987-ca6b-40bf-8626-73e1d77e904e",
   "metadata": {},
   "source": [
    "To run the container image as a container locally, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "720976ae-f83f-4674-8bc7-be2ea7328e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI:  gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}\"\n",
    "print(\"CUSTOM_PREDICTOR_IMAGE_URI: \", CUSTOM_PREDICTOR_IMAGE_URI)\n",
    "PREDICT_CONTAINER_NAME = \"local_xlm_roberta_large_xnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "654e1e3a-9644-454a-98bc-c7c40b1bf072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: local_xlm_roberta_large_xnli\n",
      "0711c1ad3b6c92ed42fadea3921e06ee7442cf6567c1ee7abe35b9b05bc3a677\n"
     ]
    }
   ],
   "source": [
    "!docker stop $PREDICT_CONTAINER_NAME\n",
    "!docker run -t -d --rm -p 7080:7080 --name=$PREDICT_CONTAINER_NAME $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97a29c2c-8b31-4b55-9fc0-caf2f563ea3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To send the container's server a health check, run the following command:\n",
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad97635-f7f5-48f9-89d4-85c7248e96bd",
   "metadata": {},
   "source": [
    "To send the container's server a prediction request, run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cc0385af-c0c8-4d7e-9e52-344d2008ebb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg=='"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = b\"You aren't kind, i hate you.\"\n",
    "b64_encoded = base64.b64encode(instance)\n",
    "b64_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "38a4bdf6-9678-46c8-9ab6-63a57667558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\": [{\"sequence\": \"You aren't kind, i hate you.\", \"labels\": [\"negative\", \"neutral\", \"positive\"], \"scores\": [0.9942014217376709, 0.0030435377266258, 0.0027550666127353907]}]}"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./predictor/instances.json <<END\n",
    "{\n",
    "   \"instances\": [\n",
    "     {\n",
    "       \"data\": {\n",
    "         \"b64\": \"WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg==\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "982234de-0672-4e3e-b8eb-330a99426e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b64_encoded:  b'WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg=='\n",
      "b64_encoded.decode:  WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg==\n",
      "test_instance:  {'instances': [{'data': {'b64': 'WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg=='}}]}\n",
      "json.dumps(test_instance):  {\"instances\": [{\"data\": {\"b64\": \"WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg==\"}}]}\n"
     ]
    }
   ],
   "source": [
    "# Other way\n",
    "instance = b\"You aren't kind, i hate you.\"\n",
    "b64_encoded = base64.b64encode(instance)\n",
    "print(\"b64_encoded: \", b64_encoded)\n",
    "print(\"b64_encoded.decode: \", b64_encoded.decode('utf-8'))\n",
    "test_instance = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"b64\": b64_encoded.decode('utf-8')\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"test_instance: \", test_instance)\n",
    "print(\"json.dumps(test_instance): \", json.dumps(test_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "521f7c32-5b9e-4165-ac51-90bcde6cfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c91c9cab-0c06-4912-9dd4-43f40eb256b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'sequence': \"You aren't kind, i hate you.\",\n",
       "   'labels': ['negative', 'neutral', 'positive'],\n",
       "   'scores': [0.9942014217376709, 0.0030435377266258, 0.0027550666127353907]}]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# payload = '{\"instances\":[{\"data\": {\"b64\": \"WW91IGFyZW4ndCBraW5kLCBpIGhhdGUgeW91Lg==\"}}]}'\n",
    "payload = json.dumps(test_instance)\n",
    "r = requests.post(\n",
    "    f\"http://localhost:7080/predictions/{APP_NAME}/\",\n",
    "    headers={\"Content-Type\": \"application/json\", \"charset\": \"utf-8\"},\n",
    "    data=payload\n",
    ")\n",
    "\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c5b57734-7d27-44ff-a2cd-81932dbfa285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_xlm_roberta_large_xnli\n"
     ]
    }
   ],
   "source": [
    "# Stop the container\n",
    "!docker stop $PREDICT_CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f7d96bb-f1ce-4534-aba5-44ad9646670f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: local_xlm_roberta_large_xnli\n",
      "Error: failed to start containers: local_xlm_roberta_large_xnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0513 14:17:41.946436689   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "# Start the container\n",
    "!docker start $PREDICT_CONTAINER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e670d780-c0db-4735-bb67-a3f503b172ae",
   "metadata": {},
   "source": [
    "# Deploying the serving container to Vertex AI Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf991a7-8b13-4792-8a7e-a320a83af2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f835de7c-e1af-4e09-a9d6-02c47e4afd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli]\n",
      "\n",
      "\u001b[1Bee34fa11: Preparing \n",
      "\u001b[1Bdd45180b: Preparing \n",
      "\u001b[1B22f0ef5f: Preparing \n",
      "\u001b[1B69d9d12d: Preparing \n",
      "\u001b[1Ba78796d0: Preparing \n",
      "\u001b[1B7ae06b2c: Preparing \n",
      "\u001b[1B7f4f2867: Preparing \n",
      "\u001b[1B80252ad2: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B6e366763: Preparing \n",
      "\u001b[1Bfbe2a476: Preparing \n",
      "\u001b[1B4d4eeb09: Preparing \n",
      "\u001b[1B3c9adfa3: Preparing \n",
      "\u001b[1B28ad53a6: Preparing \n",
      "\u001b[1B6b650202: Preparing \n",
      "\u001b[1Bf2b9b970: Preparing \n",
      "\u001b[13B78796d0: Pushed   2.262GB/2.262GB\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[12A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[10A\u001b[2K\u001b[17A\u001b[2K\u001b[10A\u001b[2K\u001b[17A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[8A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[13A\u001b[2K\u001b[3A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2KPushing  446.2MB/871.7MB\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2K\u001b[2A\u001b[2K\u001b[13A\u001b[2KPushing  569.9MB/871.7MB\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[2A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2Klatest: digest: sha256:a14187f385b95490e44fdaae4950dc1e1e145c3b83e12a16f5eec58ac2298ac7 size: 3878\n"
     ]
    }
   ],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6302f7c-8448-419a-8d01-2e2355fb00af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Successfully resolved tag to sha256, but it is recommended to use sha256 directly.\n",
      "image_summary:\n",
      "  digest: sha256:a14187f385b95490e44fdaae4950dc1e1e145c3b83e12a16f5eec58ac2298ac7\n",
      "  fully_qualified_digest: gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli@sha256:a14187f385b95490e44fdaae4950dc1e1e145c3b83e12a16f5eec58ac2298ac7\n",
      "  registry: gcr.io\n",
      "  repository: ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli\n"
     ]
    }
   ],
   "source": [
    "# Validate the custom container image in Container Registry\n",
    "!gcloud container images describe $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8323b1e-b1c0-4ed7-b6ad-8653becafdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: ideta-ml-thi\n",
      "BUCKET_NAME: gs://ideta-sentiment-analysis/\n",
      "REGION: europe-west1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"BUCKET_NAME: {BUCKET_NAME}\")\n",
    "print(f\"REGION: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cd29095-2493-41c9-969c-df6862f5dd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b118dfa-8824-421d-bc6a-724f0a9a6bc1",
   "metadata": {},
   "source": [
    "# Create a Model resource with custom serving container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0f669e-a43c-488f-9826-ad01d8a85e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "model_description = \"PyTorch based sentiment analysis with 3 labels\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e1de9e1-94ac-4e16-a8c5-43a5c6c66cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/211294546736/locations/europe-west1/models/7738046176938688512/operations/4262652649259663360\n",
      "Model created. Resource name: projects/211294546736/locations/europe-west1/models/7738046176938688512\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/211294546736/locations/europe-west1/models/7738046176938688512')\n",
      "pt-xlm-roberta-large-xnli-v1\n",
      "projects/211294546736/locations/europe-west1/models/7738046176938688512\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de249235-2f60-4b5a-9dd0-739527a64ba9",
   "metadata": {},
   "source": [
    "# Create an Endpoint for Model with Custom Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02885366-479f-405b-a478-15ea2ae8e903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt-xlm-roberta-large-xnli-endpoint'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint_display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74d78265-29e1-4c44-b5da-72cdc259f119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/211294546736/locations/europe-west1/endpoints/4667664354420719616/operations/4070123765189574656\n",
      "Endpoint created. Resource name: projects/211294546736/locations/europe-west1/endpoints/4667664354420719616\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/211294546736/locations/europe-west1/endpoints/4667664354420719616')\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dde10f6-ed05-4c1c-8f2c-2540cb7bc19d",
   "metadata": {},
   "source": [
    "# Deploy the model to endpoint\n",
    "\n",
    "__Note__: This step takes few minutes to deploy the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a8483b-91e3-4f83-9ab6-e885dd97946e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt-xlm-roberta-large-xnli-v1'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4301ce53-8686-4da0-b7e7-9fbe0c5dfc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/211294546736/locations/europe-west1/endpoints/4667664354420719616\n",
      "Deploy Endpoint model backing LRO: projects/211294546736/locations/europe-west1/endpoints/4667664354420719616/operations/7171978008541003776\n",
      "Endpoint model deployed. Resource name: projects/211294546736/locations/europe-west1/endpoints/4667664354420719616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7fb95f272a50> \n",
       "resource name: projects/211294546736/locations/europe-west1/endpoints/4667664354420719616"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 3\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddd5f42-3418-4078-9a19-87d192da3b92",
   "metadata": {},
   "source": [
    "# Invoking the Endpoint with deployed Model using Vertex AI SDK to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b76094d-fae8-45ba-94d8-5f7e3b8384ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint display name = pt-xlm-roberta-large-xnli-endpoint resource id =projects/211294546736/locations/europe-west1/endpoints/4667664354420719616 \n"
     ]
    }
   ],
   "source": [
    "# Get the list of endpoints\n",
    "filter = f'display_name=\"{endpoint_display_name}\"'\n",
    "\n",
    "for endpoint_info in aiplatform.Endpoint.list(filter=filter):\n",
    "    print(\n",
    "        f\"Endpoint display name = {endpoint_info.display_name} resource id ={endpoint_info.resource_name} \"\n",
    "    )\n",
    "\n",
    "endpoint = aiplatform.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c9e3eb1-3ce0-4f94-9b36-a6dc21a78c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"1498871843170287616\"\n",
       "model: \"projects/211294546736/locations/europe-west1/models/7738046176938688512\"\n",
       "display_name: \"pt-xlm-roberta-large-xnli-v1\"\n",
       "create_time {\n",
       "  seconds: 1652449695\n",
       "  nanos: 672793000\n",
       "}\n",
       "dedicated_resources {\n",
       "  machine_spec {\n",
       "    machine_type: \"n1-standard-4\"\n",
       "  }\n",
       "  min_replica_count: 1\n",
       "  max_replica_count: 1\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List models wrt. this endpoint\n",
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326ddc17-54e0-4cc9-a87b-40d74e4e4d90",
   "metadata": {},
   "source": [
    "## Make some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81edf356-283f-48c7-80a4-15aa818f15b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_instance: [{'data': {'b64': 'SSBhbSByZWFsbHkgZGlzYXBvaW50ZWQgd2l0aCB5b3VyIHNlcnZpY2Uh'}}]\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Rate of traffic exceeds capacity. Ramp your traffic up more slowly. endpoint_id: 4667664354420719616, deployed_model_id: 1498871843170287616.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.RESOURCE_EXHAUSTED\n\tdetails = \"Rate of traffic exceeds capacity. Ramp your traffic up more slowly. endpoint_id: 4667664354420719616, deployed_model_id: 1498871843170287616.\"\n\tdebug_error_string = \"{\"created\":\"@1652453808.807313721\",\"description\":\"Error received from peer ipv4:64.233.184.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":903,\"grpc_message\":\"Rate of traffic exceeds capacity. Ramp your traffic up more slowly. endpoint_id: 4667664354420719616, deployed_model_id: 1498871843170287616.\",\"grpc_status\":8}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14264/2002948602.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"b64\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{str(b64_encoded.decode('utf-8'))}\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test_instance: {test_instance}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Prediction response: \\n\\t{prediction}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, instances, parameters, timeout)\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m         )\n\u001b[1;32m   1270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         )\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhausted\u001b[0m: 429 Rate of traffic exceeds capacity. Ramp your traffic up more slowly. endpoint_id: 4667664354420719616, deployed_model_id: 1498871843170287616."
     ]
    }
   ],
   "source": [
    "# Single sentence\n",
    "instance = b\"I am really disapointed with your service!\"\n",
    "b64_encoded = base64.b64encode(instance)\n",
    "test_instance = [{\"data\": {\"b64\": f\"{str(b64_encoded.decode('utf-8'))}\"}}]\n",
    "print(f\"test_instance: {test_instance}\")\n",
    "prediction = endpoint.predict(instances=test_instance)\n",
    "print(f\"Prediction response: \\n\\t{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3d793-f81d-44e5-9a44-6450026c273a",
   "metadata": {},
   "source": [
    "You can use this site to encode text in 64: https://www.base64encode.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5dfe6164-0034-4a3a-9a45-a03c5be32799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction response: \n",
      "\tPrediction(predictions=[{'scores': [0.9699267745018005, 0.02488462254405022, 0.005188622046262026], 'sequence': 'I am happy', 'labels': ['positive', 'neutral', 'negative']}], deployed_model_id='1498871843170287616', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "# \"I am happy\"\n",
    "test_instance = [{\"data\": {\"b64\": \"SSBhbSBoYXBweQ==\"}}]\n",
    "prediction = endpoint.predict(instances=test_instance)\n",
    "print(f\"Prediction response: \\n\\t{prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1d17e6-719d-464d-a50f-99f9371d65e1",
   "metadata": {},
   "source": [
    "# (Fix 429) Try with `custom_handler_2.py` and `Dockerfile.2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5633d3-c26e-4a0a-b8a1-0675b442a044",
   "metadata": {},
   "source": [
    "## Option 2: Using `tokenizer` to encode and then decode to text to be used in `inference` with `pipeline()`\n",
    "\n",
    "Load the model first => this section: \"Load from saved model\"\n",
    "\n",
    "__Result__: \n",
    "\n",
    "- the problem are still there!\n",
    "- __Solution__: When creating a new endpoint, set \"Maximum number of compute nodes\" to a number (don't leave it empty) and also choose a more powerful \"Machine type\".\n",
    "- This section contains useful (and also final) codes for `Dockerfile` and `custom_handler.py` which are used in constructing the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad086f-2f99-4fad-89a6-c9c596e54235",
   "metadata": {},
   "source": [
    "### Encode and decode without `return_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601c182b-a89c-4f26-8594-33a8090db6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 87, 25, 39, 6183, 6392, 38496, 297, 678, 2367, 621, 398, 54433, 136, 935, 4516, 4, 8337, 759, 17265, 4420, 11305, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_2 = saved_tokenizer(\"I\\'m really disapointed with what are you saying and your service, give my money back!!!!\")\n",
    "inputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aed49c90-f759-41ba-ab74-941a1755306b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f919100c-b686-4af2-b20a-33bd75efc7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really disapointed with what are you saying and your service, give my money back!!!!\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_tokenizer.decode(inputs_2[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c314643-4855-4cec-a896-8121b0a912ec",
   "metadata": {},
   "source": [
    "### Ecode and decode with `return_tensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "336fe329-5d6a-4549-affa-63551fee2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I\\'m really disapointed with what are you saying and your service, give my money back!!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a6e2f11-97ae-4118-a905-70fafa465535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,    87,    25,    39,  6183,  6392, 38496,   297,   678,  2367,\n",
       "           621,   398, 54433,   136,   935,  4516,     4,  8337,   759, 17265,\n",
       "          4420, 11305,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = saved_tokenizer(sentence, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b79853fe-1902-4d3d-89f8-4c3f8faf4b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18080/940421061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaved_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3309\u001b[0m         )\n\u001b[1;32m   3310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "saved_tokenizer.decode(inputs[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f957be7-b9d7-4a6c-9609-d6d777f245d5",
   "metadata": {},
   "source": [
    "__WHY???__\n",
    "\n",
    "Try with manually convert to tensor using `torch.tensor` instead of using `return_tensors=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16f2d734-b229-4b6e-99c1-960ab7eacd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 87, 25, 39, 6183, 6392, 38496, 297, 678, 2367, 621, 398, 54433, 136, 935, 4516, 4, 8337, 759, 17265, 4420, 11305, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_3 = saved_tokenizer(\"I\\'m really disapointed with what are you saying and your service, give my money back!!!!\")\n",
    "inputs_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8064dbe7-9de4-4c6c-aa81-64c9e608caa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,    87,    25,    39,  6183,  6392, 38496,   297,   678,  2367,\n",
       "          621,   398, 54433,   136,   935,  4516,     4,  8337,   759, 17265,\n",
       "         4420, 11305,     2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_inputs = torch.tensor(inputs_3[\"input_ids\"])\n",
    "new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b1f278f1-05f8-423d-b75a-d8352dccc571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really disapointed with what are you saying and your service, give my money back!!!!\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_tokenizer.decode(new_inputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04af33-8d04-4948-8b3d-51cfe55f83ed",
   "metadata": {},
   "source": [
    "Try with get inside the `inputs[\"input_ids\"]` => bypass the dimension problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23fa0b92-cd17-474f-bda2-eb6d033887a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really disapointed with what are you saying and your service, give my money back!!!!\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1b53c-b1ca-44f4-8eb5-4a6e38cb4f94",
   "metadata": {},
   "source": [
    "### Build another image\n",
    "\n",
    "👉 The only difference is the return of function `preprocess()` in `custom_handler_3.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "839ee993-e1fe-4f76-807c-65d21e5999b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/ideta-ml-thi/pt-xlm-roberta-large-xnli_3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{APP_NAME}_3\"\n",
    "CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80e7fa3d-7a34-4e5a-a9c7-1486abcef29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.262GB\n",
      "Step 1/15 : FROM pytorch/torchserve:latest-cpu\n",
      " ---> a1d88b873573\n",
      "Step 2/15 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 9cd7983a39ee\n",
      "Step 3/15 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> 05d7d7d7e863\n",
      "Step 4/15 : USER model-server\n",
      " ---> Using cache\n",
      " ---> c989f33b4974\n",
      "Step 5/15 : COPY ./custom_handler_3.py /home/model-server/\n",
      " ---> c467df415b43\n",
      "Step 6/15 : COPY ./model/pt-xlm-roberta-large-xnli/ /home/model-server/\n",
      " ---> d3b7e1fd9f66\n",
      "Step 7/15 : USER root\n",
      " ---> Running in 0b97f1e0a77c\n",
      "Removing intermediate container 0b97f1e0a77c\n",
      " ---> f6ec2600c937\n",
      "Step 8/15 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in 08324f39afdf\n",
      "Removing intermediate container 08324f39afdf\n",
      " ---> 172e3c2d18de\n",
      "Step 9/15 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in 099b27fc529f\n",
      "Removing intermediate container 099b27fc529f\n",
      " ---> 9088720233cd\n",
      "Step 10/15 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in 728e654a77dc\n",
      "Removing intermediate container 728e654a77dc\n",
      " ---> b73d07e92b3c\n",
      "Step 11/15 : USER model-server\n",
      " ---> Running in f98f2e629333\n",
      "Removing intermediate container f98f2e629333\n",
      " ---> 1cd977985a20\n",
      "Step 12/15 : EXPOSE 7080\n",
      " ---> Running in 31eca39fd365\n",
      "Removing intermediate container 31eca39fd365\n",
      " ---> 2f92128e79ed\n",
      "Step 13/15 : EXPOSE 7081\n",
      " ---> Running in c8eda6726ca2\n",
      "Removing intermediate container c8eda6726ca2\n",
      " ---> 6867a67ab673\n",
      "Step 14/15 : RUN torch-model-archiver -f   --model-name=pt-xlm-roberta-large-xnli   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler_3.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in 273ceb935163\n",
      "Removing intermediate container 273ceb935163\n",
      " ---> 415205252e7f\n",
      "Step 15/15 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"pt-xlm-roberta-large-xnli=pt-xlm-roberta-large-xnli.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in 2e32a9b4f4d1\n",
      "Removing intermediate container 2e32a9b4f4d1\n",
      " ---> 77c0ff991943\n",
      "Successfully built 77c0ff991943\n",
      "Successfully tagged gcr.io/ideta-ml-thi/pt-xlm-roberta-large-xnli_3:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build --tag=$CUSTOM_PREDICTOR_IMAGE_URI ./predictor -f ./predictor/Dockerfile.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98428bbf-4dea-4047-a45b-05e34464de7b",
   "metadata": {},
   "source": [
    "### Create another container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e255f8a-d2db-42d6-a692-394bfe47d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: local_xlm_roberta_large_xnli_3\n",
      "9aa2ff237197b1a3bd3e90eb823ddd78baba7e509d432ccfe706755a0094c7d8\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/{APP_NAME}_3\"\n",
    "PREDICT_CONTAINER_NAME = \"local_xlm_roberta_large_xnli_3\"\n",
    "!docker stop $PREDICT_CONTAINER_NAME\n",
    "!docker run -t -d --rm -p 7080:7080 --name=$PREDICT_CONTAINER_NAME $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543b66cd-ae56-4c10-83b0-d635984e1707",
   "metadata": {},
   "source": [
    "### Check healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5535b29e-2417-433a-917f-61a5da320115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b7c13-47ea-484d-a068-9f0499bede1f",
   "metadata": {},
   "source": [
    "### Make a prediction locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a7aee97-4ccc-4333-834d-febe641ab2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'sequence': \"You aren't kind, i hate you.\",\n",
       "   'labels': ['negative', 'neutral', 'positive'],\n",
       "   'scores': [0.9942014217376709, 0.0030435377266258, 0.0027550666127353907]}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = b\"You aren't kind, i hate you.\"\n",
    "b64_encoded = base64.b64encode(instance)\n",
    "test_instance = {\n",
    "    \"instances\": [\n",
    "        {\n",
    "            \"data\": {\n",
    "                \"b64\": b64_encoded.decode('utf-8')\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "payload = json.dumps(test_instance)\n",
    "r = requests.post(\n",
    "    f\"http://localhost:7080/predictions/{APP_NAME}/\",\n",
    "    headers={\"Content-Type\": \"application/json\", \"charset\": \"utf-8\"},\n",
    "    data=payload\n",
    ")\n",
    "\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfd500-e01a-4682-b628-9da1ee477878",
   "metadata": {},
   "source": [
    "### Deploy to vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0592fdd-0a4f-44b7-a8dd-28e5309486a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gcr.io/ideta-ml-thi/pt-xlm-roberta-large-xnli_3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ae336ab-44a6-4a5c-9da5-4dd4137937d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/ideta-ml-thi/pt-xlm-roberta-large-xnli_3]\n",
      "\n",
      "\u001b[1B1a791db3: Preparing \n",
      "\u001b[1B86b98fc6: Preparing \n",
      "\u001b[1B4d03d205: Preparing \n",
      "\u001b[1Baf584099: Preparing \n",
      "\u001b[1Be2f2374e: Preparing \n",
      "\u001b[1B6936b933: Preparing \n",
      "\u001b[1B7f4f2867: Preparing \n",
      "\u001b[1B80252ad2: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B6e366763: Preparing \n",
      "\u001b[1Bfbe2a476: Preparing \n",
      "\u001b[7B6936b933: Waiting g \n",
      "\u001b[6B80252ad2: Waiting g \n",
      "\u001b[1B28ad53a6: Preparing \n",
      "\u001b[4B4d4eeb09: Waiting g \n",
      "\u001b[4B3c9adfa3: Waiting g \n",
      "\u001b[13B2f2374e: Pushed   2.262GB/2.262GB\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[17A\u001b[2K\u001b[11A\u001b[2K\u001b[17A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[7A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[4A\u001b[2K\u001b[13A\u001b[2K\u001b[3A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[12A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2K\u001b[13A\u001b[2Klatest: digest: sha256:d1a589c433fb561d9bd6ed5141f82a424e62375e3d11c9165f0d2020c049d473 size: 3878\n"
     ]
    }
   ],
   "source": [
    "!docker push $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03f130ba-4625-45c1-89a1-3e8471bd7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Successfully resolved tag to sha256, but it is recommended to use sha256 directly.\n",
      "image_summary:\n",
      "  digest: sha256:d1a589c433fb561d9bd6ed5141f82a424e62375e3d11c9165f0d2020c049d473\n",
      "  fully_qualified_digest: gcr.io/ideta-ml-thi/pt-xlm-roberta-large-xnli_3@sha256:d1a589c433fb561d9bd6ed5141f82a424e62375e3d11c9165f0d2020c049d473\n",
      "  registry: gcr.io\n",
      "  repository: ideta-ml-thi/pt-xlm-roberta-large-xnli_3\n"
     ]
    }
   ],
   "source": [
    "# Validate the custom container image in Container Registry\n",
    "!gcloud container images describe $CUSTOM_PREDICTOR_IMAGE_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d6dc548-7246-4f66-8c57-49d373164762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: ideta-ml-thi\n",
      "BUCKET_NAME: gs://ideta-sentiment-analysis/\n",
      "REGION: europe-west1\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"BUCKET_NAME: {BUCKET_NAME}\")\n",
    "print(f\"REGION: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb825b3e-f702-43fa-a663-28f45812f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a7a8c9e-b7d9-4a23-9491-b1ee699900b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_display_name:  pt-xlm-roberta-large-xnli-v2\n"
     ]
    }
   ],
   "source": [
    "VERSION = 2\n",
    "model_display_name = f\"{APP_NAME}-v{VERSION}\"\n",
    "print(\"model_display_name: \", model_display_name)\n",
    "model_description = \"PyTorch based sentiment analysis with 3 labels\"\n",
    "\n",
    "MODEL_NAME = APP_NAME\n",
    "health_route = \"/ping\"\n",
    "predict_route = f\"/predictions/{MODEL_NAME}\"\n",
    "serving_container_ports = [7080]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "731d9445-b17b-4387-bd81-30442d7232cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/211294546736/locations/europe-west1/models/8348283926447390720/operations/5826815691910545408\n",
      "Model created. Resource name: projects/211294546736/locations/europe-west1/models/8348283926447390720\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/211294546736/locations/europe-west1/models/8348283926447390720')\n",
      "pt-xlm-roberta-large-xnli-v2\n",
      "projects/211294546736/locations/europe-west1/models/8348283926447390720\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d30e12-47c4-4f8a-90b7-217c47fd60e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8d367-2d2c-4efe-8546-99a3ffd2b7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b52373-1895-43ea-9fac-f0504a5664fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71c331ed-b524-4221-baac-029ee5d12c13",
   "metadata": {},
   "source": [
    "## Option 1: The return of `preprocess` = original input sentences\n",
    "\n",
    "👉 The only difference are functions `preprocess()` and `inference()` in `custom_handler_2.py`\n",
    "\n",
    "```python\n",
    "def preprocess(self, data):\n",
    "    \"\"\" Preprocessing input request by tokenizing\n",
    "        Extend with your own preprocessing steps as needed\n",
    "    \"\"\"\n",
    "    text = data[0].get(\"data\")\n",
    "    if text is None:\n",
    "        text = data[0].get(\"body\")\n",
    "    sentences = text.decode('utf-8')\n",
    "    logger.info(\"Received text: '%s'\", sentences)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def inference(self, inputs):\n",
    "    \"\"\" Predict the class of a text using a trained transformer model.\n",
    "    \"\"\"\n",
    "    prediction = self.pipe(inputs[0], candidate_labels=[\"negative\", \"neutral\", \"positive\"])\n",
    "    if len(inputs) == 1:\n",
    "        prediction = [prediction]\n",
    "    return prediction\n",
    "```\n",
    "\n",
    "👉 __Result__: fail, check the log of error at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "02c42b45-03fd-416c-baae-112f7438912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0513 21:04:57.926380585   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  2.262GB\n",
      "Step 1/15 : FROM pytorch/torchserve:latest-cpu\n",
      " ---> a1d88b873573\n",
      "Step 2/15 : RUN python3 -m pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 9cd7983a39ee\n",
      "Step 3/15 : RUN pip3 install transformers\n",
      " ---> Using cache\n",
      " ---> 05d7d7d7e863\n",
      "Step 4/15 : USER model-server\n",
      " ---> Using cache\n",
      " ---> c989f33b4974\n",
      "Step 5/15 : COPY ./custom_handler_2.py /home/model-server/\n",
      " ---> e961e600e47c\n",
      "Step 6/15 : COPY ./model/pt-xlm-roberta-large-xnli/ /home/model-server/\n",
      " ---> cb6519ae252b\n",
      "Step 7/15 : USER root\n",
      " ---> Running in 5e725a9ffdf1\n",
      "Removing intermediate container 5e725a9ffdf1\n",
      " ---> 751cdd0d5851\n",
      "Step 8/15 : RUN printf \"\\nservice_envelope=json\" >> /home/model-server/config.properties\n",
      " ---> Running in 2ba3ae6f9932\n",
      "Removing intermediate container 2ba3ae6f9932\n",
      " ---> 880a5c82453f\n",
      "Step 9/15 : RUN printf \"\\ninference_address=http://0.0.0.0:7080\" >> /home/model-server/config.properties\n",
      " ---> Running in cd4f32c0f355\n",
      "Removing intermediate container cd4f32c0f355\n",
      " ---> b6c2c935ee1d\n",
      "Step 10/15 : RUN printf \"\\nmanagement_address=http://0.0.0.0:7081\" >> /home/model-server/config.properties\n",
      " ---> Running in b13e8227f7f7\n",
      "Removing intermediate container b13e8227f7f7\n",
      " ---> f053e6eae4a2\n",
      "Step 11/15 : USER model-server\n",
      " ---> Running in c12dc4f1f71a\n",
      "Removing intermediate container c12dc4f1f71a\n",
      " ---> 19b12879a059\n",
      "Step 12/15 : EXPOSE 7080\n",
      " ---> Running in 654bb34c4c1a\n",
      "Removing intermediate container 654bb34c4c1a\n",
      " ---> 62616b83c600\n",
      "Step 13/15 : EXPOSE 7081\n",
      " ---> Running in 28340370146b\n",
      "Removing intermediate container 28340370146b\n",
      " ---> 6af6a5d7148a\n",
      "Step 14/15 : RUN torch-model-archiver -f   --model-name=pt-xlm-roberta-large-xnli   --version=1.0   --serialized-file=/home/model-server/pytorch_model.bin   --handler=/home/model-server/custom_handler_2.py   --extra-files \"/home/model-server/config.json,/home/model-server/tokenizer.json,/home/model-server/tokenizer_config.json,/home/model-server/special_tokens_map.json\"   --export-path=/home/model-server/model-store\n",
      " ---> Running in c2a32b6b4cbd\n",
      "Removing intermediate container c2a32b6b4cbd\n",
      " ---> 0c0b122f14d7\n",
      "Step 15/15 : CMD [\"torchserve\",      \"--start\",      \"--ts-config=/home/model-server/config.properties\",      \"--models\",      \"pt-xlm-roberta-large-xnli=pt-xlm-roberta-large-xnli.mar\",      \"--model-store\",      \"/home/model-server/model-store\"]\n",
      " ---> Running in 3d623d22fd49\n",
      "Removing intermediate container 3d623d22fd49\n",
      " ---> 2a710ef931f0\n",
      "Successfully built 2a710ef931f0\n",
      "Successfully tagged gcr.io/ideta-ml-thi/pytorch_predict_pt-xlm-roberta-large-xnli_2:latest\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}_2\"\n",
    "!docker build --tag=$CUSTOM_PREDICTOR_IMAGE_URI ./predictor -f ./predictor/Dockerfile.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dffa2489-40ac-4418-95e5-8cb86a66b024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error response from daemon: No such container: local_xlm_roberta_large_xnli_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0513 21:15:07.296953339   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n",
      "E0513 21:15:07.511717375   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feac50383d5ac0d5d2c7174827112a25fb6aa6fa64cc4e9319fb89b22432d354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0513 21:15:08.552110203   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"gcr.io/{PROJECT_ID}/pytorch_predict_{APP_NAME}_2\"\n",
    "PREDICT_CONTAINER_NAME = \"local_xlm_roberta_large_xnli_2\"\n",
    "!docker stop $PREDICT_CONTAINER_NAME\n",
    "!docker run -t -d --rm -p 7080:7080 --name=$PREDICT_CONTAINER_NAME $CUSTOM_PREDICTOR_IMAGE_URI\n",
    "!sleep 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f35c778f-61eb-429d-b4e2-f8b391dc19fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0513 21:16:08.459317118   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status\": \"Healthy\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd0d5966-4f24-44b9-80f7-9e75f561dac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt-xlm-roberta-large-xnli'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14e567cf-f2e5-4729-9c85-4c2222e3754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0513 21:22:23.872905385   14264 fork_posix.cc:70]           Fork support is only compatible with the epoll1 and poll polling strategies\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"code\": 503,\n",
      "  \"type\": \"InternalServerException\",\n",
      "  \"message\": \"Prediction failed\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash -s $APP_NAME\n",
    "\n",
    "APP_NAME=$1\n",
    "\n",
    "cat > ./predictor/instances.json <<END\n",
    "{\n",
    "   \"instances\": [\n",
    "     {\n",
    "       \"data\": {\n",
    "         \"b64\": \"$(echo 'I am happy.' | base64 --wrap=0)\"\n",
    "       }\n",
    "     }\n",
    "   ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./predictor/instances.json \\\n",
    "  http://localhost:7080/predictions/$APP_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67aefaf-b354-4df1-8036-935b6bfb40d8",
   "metadata": {},
   "source": [
    "Use `docker attach <container_id>` to see the log of what are going on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b41f1-73ee-4475-a850-49d750b1c624",
   "metadata": {},
   "source": [
    "Error inside docker,\n",
    "\n",
    "```\n",
    "2022-05-13T21:22:24,590 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 668\n",
    "2022-05-13T21:22:24,589 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -   File \"/home/venv/lib/python3.8/site-packages/ts/service.py\", line 102, in predict\n",
    "2022-05-13T21:22:24,590 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0 ACCESS_LOG - /172.17.0.1:39050 \"POST /predictions/pt-xlm-roberta-large-xnli/ HTTP/1.1\" 503 670\n",
    "2022-05-13T21:22:24,591 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -     ret = self._entry_point(input_batch, self.context)\n",
    "2022-05-13T21:22:24,591 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:feac50383d5a,timestamp:null\n",
    "2022-05-13T21:22:24,592 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -   File \"/home/venv/lib/python3.8/site-packages/ts/torch_handler/request_envelope/base.py\", line 31, in handle\n",
    "2022-05-13T21:22:24,592 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -     results = self.format_output(results)\n",
    "2022-05-13T21:22:24,592 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -   File \"/home/venv/lib/python3.8/site-packages/ts/torch_handler/request_envelope/json.py\", line 24, in format_output\n",
    "2022-05-13T21:22:24,593 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -     return self._batch_to_json(data, self._lengths)\n",
    "2022-05-13T21:22:24,593 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -   File \"/home/venv/lib/python3.8/site-packages/ts/torch_handler/request_envelope/json.py\", line 60, in _batch_to_json\n",
    "2022-05-13T21:22:24,593 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG -     mini_batch = batch[cursor:cursor_end]\n",
    "2022-05-13T21:22:24,594 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_LOG - TypeError: unhashable type: 'slice'\n",
    "2022-05-13T21:22:24,594 [INFO ] W-9001-pt-xlm-roberta-large-xnli_1.0-stdout MODEL_METRICS - HandlerTime.Milliseconds:664.13|#ModelName:pt-xlm-roberta-large-xnli,Level:Model|#hostname:feac50383d5a,requestID:ab8f0b03-3b22-4a1c-aeca-6ccecf435ffd,timestamp:1652476944\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d340c5a2-cb1c-415a-8e81-05144a948b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
